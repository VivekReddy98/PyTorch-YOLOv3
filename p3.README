/*
sjoshi26 shashank joshi
akwatra archit kwatra
vkarri vivek reddy karri
*/

Q1) Does the testing job on a single GPU node reach the real-time processing speed (30 FPS)?

    Workers = 4
    Batch Size = 16
    Num Frames = 5000
    Inference Time = 64.218
    Data Loading time = 682.801 (non max suppression was around 140 secs)

    Under the assumption that processing speed = Num Frames / Data Loading time.
    Because, Just the Inference time wont be sufficient to call it Real time processing, time considered in fetching the data into memory should also be considered.

    Therefore FPS = 5000 / 682.801 = 7.322

    If the Inference time + non_max_suppression is taken into account, Then FPS = 5000 / 204.218 = 24.5

Q2) Based the break-down timing on testing, which part is the bottleneck?

    Times-for Serial Data: (num_workers = 1, batch_size = 8)
    Total Data Loading  Time Taken 692.2090117931366 secs
    Total Inference Time Taken: 70.5061936378479 secs
    Total Testing Time Taken: 779.8315935134888 secs

    So, it is clear from the results that the Data Loading part is the bottleneck in the process.
    This usually leads to a problem called "GPU Starvation", where the GPU waits for data nearly most of the time instead of doing useful work.

    On careful observation, non_max_suppression() is also taking a good amount of time, which happens on the CPU end.

    Note:
    1) Inference time is the time taken for the forward pass on the input data summed up across all the samples. We've not include non-max suppression part.
    2) Data Loading time is calculated by the inference time subtracted from total execution time for the evaluate function. Non-Max suppresion time is included in Data Loading part.

Q3) Can you provide some ideas to speedup the most time consuming part?

    1) Using Multi-Threading or Multi_processing to pre-fetch more batches of data into the memory, thus leading to fewer wasted cycles on GPU.
       (This is essentially what num_workers would do, if we are not wrong).

    2) Building Efficient Data Pipelines using Portable Data Formats specified by Libraries such as TF Records in case of Tensorflow.

    3) Increasing batch_size would help in speeding up the testing though it might lead of Out of Memory errors, when it set too high.
       So, a right tradeoff has to be found out in this regard.

    4) Maybe use MPS where multiple processes could load data into GPU memory while, one process could compute the inference.

    5) preferably save data to the local SSD or HDD instead of having global file system, since network contention could lead to bad timings.

Q4) Describe the implementation details of how you calculate final result (AP and mAP) from result of each rank?

    There are two functions, which are dealing with finding the metrics.

    1) get_batch_statistics(): This founds out true_positives, predicted_labels and prediction_Confidence Scores per image output
    2) ap_per_class(): This uses the information from these three arrays to find the Precision-Recall Curves, follwed by AP per class and mean AP.

    In two-node case, i've splitted the Data set into half and then broadcasted the results or get_batch_statistics() to rank 0.
    I've appended the results at rank 0 and then computed the AP and mean AP using ap_per_class.

    Implementational Details:
    We've Concatenated the arrays across the axis=1. Initially We've broadcasted the shapes of matrix and length of labels list.
    Then we created a necessary tensor of the shapes as specified in the shape array broadcasted.

    Then there are two broadcasts/reduce one for the concatenated matrix and one for labels arrays.

    Then, we've appended the necesary arrays with correspoding arrays on the rank 0.


Q5) Does the testing job on two GPU nodes reach the real-time processing speed (30 FPS)?

    Workers = 4
    Batch Size = 16
    Num Frames = 2500 Per Rank, 5000 in total
    Inference Time = 33.1454
    Data Loading time = 369.241 (non max suppression was around 79 secs)

    Under the assumption that processing speed = Num Frames / Data Loading time.
    Because, Just the Inference time wont be sufficient to call it Real time processing, time considered in fetching the data into memory should also be considered.

    Therefore FPS = 5000 / 369.241 = 13.541 FPS

    If Inference time + non_max_suppression is taken into account, Then FPS = 5000 / 113.1454 = 43 FPS.

Q6) Does the data parallel testing on two GPU nodes perform better than the one node scenario? Why or why not?

    Yeah it did perform well.

      Mode        Batch_Size    workers    Data_Loading_time(s)     Inference_time(s)   Total_time(s)
    Two-Node         16            4          369.241                   33.1454            424.532
    One-Node         16            4          682.801                    64.218            764.5540

    Two-Node          8            1          407.993                   35.5545            462.828
    One-Node          8            1          692.209                    70.506            779.831

    As it can be clerly seen from the Data Loading and Inference time data, the time was almost reduced to half because the problem size was made half essentially.
    Though there was an extra time seen in Two-Node case that the Total Time - Data Loading - Inference was higher than that of one-node counterpart.

    This is because of the network overhead added because of MPI communication happening.

    Note:
    1) For Two-Node case, maximum time is recorded.
    2) Inference time is the time taken for the forward pass on the input data summed up across all the samples. We've not include non-max suppression part.
    3) Data Loading time is calculated by the inference time subtracted from total execution time for the evaluate function.

Q7) Does the tuning of dataloader help? Why or why not?

      Mode        Batch_Size    workers    Data_Loading_time(s)     Inference_time(s)   Total_time(s)
    Two-Node         16            4          369.241                   33.1454            424.532
    Two-Node         16            2          396.309                   33.477             450.418

    As expected, workers had some slight impact on Data Loading time, because workers load pre-fetch them in batches.
    And was indeed slighly better in performance. num_workers = num_cores should ideally give better performance.

      Mode        Batch_Size    workers    Data_Loading_time(s)     Inference_time(s)   Total_time(s)
    Two-Node         16            4          369.241                   33.1454            424.532
    Two-Node          8            4          374.589                   35.6063            431.418

    Increasing batch size has proved to be slightly better, because the number of times we access disk would be lower and number of time, we invoke GPU for evaluation would also be low.
    However, when Batch size was set to 32, there was out of memory error.


    Other Parameters such as shuffle is only useful in training phase and is not for Speedup purposes.




Fri May  1 19:30:23 2020
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:03:00.0 Off |                  N/A |
| 27%   42C    P2    45W / 180W |   2777MiB /  8119MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     32247      C   python3                                     2767MiB |
+-----------------------------------------------------------------------------+
